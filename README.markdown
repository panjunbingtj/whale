# Whale v1.0

Whale, a novel mechanism for efficient serialization and transferring stream data between processes for one-to-many communication in DSPSs. 
Whale serializes messages before they are encapsulated into tuples in order to reduce serialize computation and transfers only once to each process to greatly
improve the communication efficiency.We implement Whale on top of Apache Storm. Experiment results show that Whale significantly improves the performance of the existing design in terms of system
throughput and process latency.
## License

Whale is released under the [Apache 2 license](http://www.apache.org/licenses/LICENSE-2.0.html).

## Introduction

By taking advantage of multi-core processors equipped machines in clusters, a distributed stream processing system (DSPS) provides efficient data parallelism, i.e., multiple instances of an operator are
created and deployed on the cluster machines to process data in parallel, to improve stream processing throughput. Accordingly, a DSPS provides diverse data partition strategies to divide the workloads
generated by an upstream instance among the downstream instances. Among the strategies, the one-to-many based partition becomes increasingly important, which broadcasts one tuple generated
by an upstream instance to many downstream instances. Such partition strategy has been widely used in many large-scale applications, such as smart ride-sharing and stork trading.

Existing DSPSs commonly implement the partition mechanisms based on instance oriented addressing, i.e., an upstream instance transmits the generated tuple to a downstream instance according to the
address of the downstream instance. The downstream instances, however, may reside on the same machine. As a result, the source node needs to send tuples containing the same data item to the same destination machine repeatedly, raising heavy serialization
and inter-server communication overhead.

We examine the performance of the above one-to-many based partition strategy using large-scale data collected from Didi Chuxing.The dataset contains 13 billion trajectory records associated
with six million drivers and 74 million order records. We conduct the experiment on top of a cluster with 30 machines, each equipped with a sixteen-core processor. The parallelism degree (i.e., the number of instances) for the order matcher operation varies from 30
to 480. Figures show the system throughput and latency decreases with theincrease of the parallelism degree.

<div align="center">

<img src="https://raw.githubusercontent.com/Tjcug/storm/master/images/storm_latency.png"  height="280px"  >
<img src="https://raw.githubusercontent.com/Tjcug/storm/master/images/storm_throughput.png" height="280px" >
<img src="https://raw.githubusercontent.com/Tjcug/storm/master/images/storm_proportion.png"  height="280px" >
</div>
 
Based on the above analysis, we observe that efficient data transmission for one-to-many based partition is the key factor that influences the performance of DSPSs. Existing instance-oriented data
transfer scheme causes heavy CPU and network consumption for one-to-many based partition. To address the problem, in this work, we propose a novel distributed stream processing system called
Whale. Whale leverages a novel worker-oriented design philosophy. Two factors contribute to the efficiency of the whale design. First, Whale exploits a novel tuple packaging strategy, which serializes the tuple to be sent to all the downstream instances only once.
Second, it employs the worker-oriented transmit scheme, which sends tuples to the worker instead of a downstream instance. We implement Whale on top of Apache Storm and conduct compressive
experiments to evaluate the performance of Whale. 

Developers and contributors should also take a look at our [Developer documentation](DEVELOPER.md).

## SYSTEM DESIGN AND IMPLEMENTATION
We implement Whale on top of Apache Storm. We completely rewrite the data communication layer of Apache Storm and add around 500 lines of java code to the Apache Storm version 2.0.0 for
Whaleâ€™s worker-oriented data communication.We make the source code public available (https://github.com/Whale-Storm/whale).

Figure shows the overview of the Whale design. The main idea of Whale is to exploit a worker-oriented design which sends tuples to workers instead of instances directly. A worker then passes tuples to the destination instances hosted on it. Accordingly, for the oneto-
many partition, Whale proposes two new components, the batch component and the dispatcher component. The batch component resides on the source worker and packages the tuples destined for
different instances on the same worker as a single batch. A batch contains one data item generated by the source task and the identifications of multiple destination instances. In the destination worker, the dispatcher component receives the batch from the source worker and dispatches the data item to the destination instances according
to the instance IDs contained in the batch. In order to reduce the computation cost, Whale serializes the data item before package the item into a tuple.

More specifically, the process of the one-to-many communication in Whale is described as follows. A source task sends an item to multiple downstream instances. First, the source task serializes
the generated item and puts it into the hosting worker. Then, the batch component in the hosting worker finds out the destination instances based on the partition strategy and package the serialized
data item with the identifications of the destination instances hosted on a same worker into a batch tuple. For a generated data item, the
number of batches is the same as the number of destination workers. Next, the source worker sends the batch to the destination worker. When the destination worker receives the batch, the dispatcher component deserializes the item and obtains the identifications of
the destination instances. Finally, the dispatcher sends the data item to the destination instances based on the identifications.

<div align="center">
<img src="https://raw.githubusercontent.com/Tjcug/storm/master/images/whale_system_desgin.png" weight="500px" height="450px" >
</div>

## Project Committers
* HanHua Chen([@chen](chen@hust.edu.cn))
* Fan Zhang([@zhangf](zhangf@hust.edu.cn))
* Jie Tan ([@tjmaster](https://tjcug.github.io/))
* Yonghui Wang([@wyh](https://github.com/WYonghui/))
* HaoPeng Jie([@jhp](https://github.com/jessezax/))

## Author and Copyright

DStream is developed in Cluster and Grid Computing Lab, Services Computing Technology and System Lab, Big Data Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China by Hanhua Chen (chen@hust.edu.cn), Fan Zhang(zhangf@hust.edu.cn), Hai Jin (hjin@hust.edu.cn), Jie Tan(tjmaster@hust.edu.cn)
Yonghui Wang(wangyonghui@hust.edu.cn),HaoPeng Jie(jhp@hust.edu.cn)

Copyright (C) 2017, [STCS & CGCL](http://grid.hust.edu.cn/) and [Huazhong University of Science and Technology](http://www.hust.edu.cn).


